{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "authorship_tag": "ABX9TyNC7s5phXyJsCXvz3hm6wf/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valsson-group/UNT-ChemicalApplicationsOfMachineLearning-Spring2026/blob/main/Lecture-10_February-24-2026/Lecture-10_BinaryClassification-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 10 - Binary Classification\n",
        "\n",
        "Here, we are going to continue to consider binary classifcation and consider the same data as in Lecture 9.\n",
        "\n",
        "We will consider data from this paper:\n",
        "- Enhancing Permeability Prediction of Heterobifunctional Degraders Using Machine Learning and Metadynamics-Informed 3D Molecular Descriptors - [DOI:10.1021/acs.jcim.5c01600](https://doi.org/10.1021/acs.jcim.5c01600)\n",
        "\n",
        "Where the authors consider the Permeability of so-called PROTAC compounds that are large and flexible molecules used in Targeted Protein Degradation.\n",
        "\n",
        "All the dataset used in the paper, and the code use to obtain the results are given in this following Github repository:\n",
        "- https://github.com/brykimjh/degrader-permeability-ml3d-metaD  \n",
        "\n",
        "The specfic dataset that we use 32 PROTACs with measured passive permeability (given in nm/s) and includes 17 features calculated by RDKit (see [here](https://github.com/brykimjh/degrader-permeability-ml3d-metaD/blob/main/data/calculate_2d_properties.py) for the script they are calculated)\n",
        "\n",
        "The target value is the measured passive permeability that is experimentaly measured.\n",
        "\n",
        "The dataset can be seen here:\n",
        "- https://github.com/brykimjh/degrader-permeability-ml3d-metaD/blob/main/data/2d_features.csv\n",
        "\n",
        "Where the assive permeability is given by `P_app`"
      ],
      "metadata": {
        "id": "aAzSy5lwWyxC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vudPPHEbxOwZ"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "\n",
        "%%bash\n",
        "dataset_url=\"https://raw.githubusercontent.com/brykimjh/degrader-permeability-ml3d-metaD/refs/heads/main/data/2d_features.csv\"\n",
        "wget ${dataset_url}\n",
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "MrAhw83Cxzw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"2d_features.csv\")"
      ],
      "metadata": {
        "id": "pUY06gC2x_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now turn the problem into a classification problem by seperating the molecules into molecules with high permeability and low permeability, by using a cutoff of 7 nm/s that will split the data set equally.\n",
        "\n"
      ],
      "metadata": {
        "id": "DUBh_h0uaSWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Permeable_cutoff = 7.0\n",
        "Low_label = 0\n",
        "High_label = +1\n",
        "Permeable_key_str = f'Permeability High({High_label:})/Low({Low_label:})'\n",
        "dataset[Permeable_key_str] = [High_label if p > Permeable_cutoff else Low_label for p in dataset['P_app']]\n",
        "\n",
        "Number_Permeable_High = np.sum(dataset[Permeable_key_str] == +1)\n",
        "Number_Permeable_Low = np.sum(dataset[Permeable_key_str] == 0)\n",
        "\n",
        "print(\"Key:\",Permeable_key_str)\n",
        "\n",
        "print(\"Number with high permeability (above {:.1f} nm/s): {:d}\".format(Permeable_cutoff,Number_Permeable_High))\n",
        "print(\"Number with low permeability (above {:.1f} nm/s): {:d}\".format(Permeable_cutoff,Number_Permeable_Low))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "dataset[['P_app', Permeable_key_str] ]"
      ],
      "metadata": {
        "id": "9ehEH7h02xgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.keys())"
      ],
      "metadata": {
        "id": "GDzjprGynFfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a data frame with just the features and target values\n",
        "features = dataset.drop(columns=['Index',\n",
        "                                 'Compound',\n",
        "                                 'P_app AB (nm/s)',\n",
        "                                 'P_app BA (nm/s)',\n",
        "                                 'P_app',\n",
        "                                 'Smiles',\n",
        "                                 'Permeability High(1)/Low(0)'])\n",
        "target = dataset['Permeability High(1)/Low(0)']"
      ],
      "metadata": {
        "id": "gRYAXiOi36cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "jOVV67vYbHCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "id": "AyWFSgXRbNRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k Nearest Neighbors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YzGctE0Cc03a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate,ShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.5)\n",
        "\n",
        "n_neighbors=5\n",
        "\n",
        "model = Pipeline(\n",
        "    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors))]\n",
        ")\n",
        "\n",
        "model.fit(features_train,target_train)\n",
        "\n",
        "target_test_predicted = model.predict(features_test)\n",
        "\n",
        "print(\"Accuracy:                 {:.4f}\".format(metrics.accuracy_score(target_test,target_test_predicted)))\n",
        "print(\"Precision:                {:.4f}\".format(metrics.precision_score(target_test,target_test_predicted)))\n",
        "print(\"Recall:                   {:.4f}\".format(metrics.recall_score(target_test,target_test_predicted)))\n",
        "\n",
        "cfm = metrics.ConfusionMatrixDisplay.from_predictions(target_test,target_test_predicted)\n"
      ],
      "metadata": {
        "id": "wg39_uF2k86M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate,ShuffleSplit,StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "n_neighbors=5\n",
        "\n",
        "model = Pipeline(\n",
        "    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors))]\n",
        ")\n",
        "\n",
        "\n",
        "scoring = {'accuracy':'accuracy',\n",
        "            'recall': metrics.make_scorer(metrics.recall_score, zero_division=np.nan),\n",
        "            'precision': metrics.make_scorer(metrics.precision_score, zero_division=np.nan),\n",
        "           'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "\n",
        "# employ 5-fold CV\n",
        "scores_fold = cross_validate(\n",
        "    model,\n",
        "    features, target,\n",
        "    scoring=scoring,\n",
        "    cv=StratifiedKFold(n_splits=4, shuffle=True),\n",
        "    return_train_score=True,\n",
        "    return_estimator=True,\n",
        "    return_indices=True\n",
        ")\n",
        "\n",
        "# Evaluate the models using crossvalidation\n",
        "NumSplits=100\n",
        "cv_random = ShuffleSplit(n_splits=NumSplits, test_size=0.5)\n",
        "scores_random = cross_validate(\n",
        "    model,\n",
        "    features, target,\n",
        "    scoring=scoring,\n",
        "    cv=cv_random,\n",
        "    return_train_score=True,\n",
        "    return_estimator=True,\n",
        "    return_indices=True\n",
        ")\n",
        "\n",
        "# metrics.RocCurveDisplay.from_cv_results(scores_random,\n",
        "#                                         features,\n",
        "#                                         target)\n",
        "\n",
        "\n",
        "print(\"Accuracy - Test\")\n",
        "print(\"- 5-Fold CV                   : {:.3f} +- {:.3f}\".format(scores_fold['test_accuracy'].mean(),scores_fold['test_accuracy'].std()))\n",
        "print(\"- Random Splits ({:d} splits) : {:.3f} +- {:.3f}\".format(NumSplits, scores_random['test_accuracy'].mean(), scores_random['test_accuracy'].std()))\n",
        "\n",
        "print(\"ROC AUC - Test\")\n",
        "print(\"- 5-Fold CV                   : {:.3f} +- {:.3f}\".format(scores_fold['test_roc_auc'].mean(),scores_fold['test_roc_auc'].std()))\n",
        "print(\"- Random Splits ({:d} splits) : {:.3f} +- {:.3f}\".format(NumSplits, scores_random['test_roc_auc'].mean(), scores_random['test_roc_auc'].std()))\n",
        "\n",
        "print(\"Precision\")\n",
        "print(\"- 5-Fold CV                   : {:.3f} +- {:.3f}\".format( np.nanmean(scores_fold['test_precision']),np.nanstd(scores_fold['test_precision'])))\n",
        "print(\"- Random Splits ({:d} splits) : {:.3f} +- {:.3f}\".format(NumSplits, np.nanmean(scores_random['test_precision']), np.nanstd(scores_random['test_precision'])))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "twX6Gz-H4Qlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}